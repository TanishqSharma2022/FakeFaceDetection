{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1632\n",
      "Test dataset size: 409\n",
      "Batch of images shape: torch.Size([32, 3, 128, 128])\n",
      "Batch of labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "# Define the transformations (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize all images to 128x128\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (ImageNet stats)\n",
    "])\n",
    "\n",
    "# Define the paths to the dataset directories\n",
    "train_dir = 'CNN_dataset/train'  # Path to the train folder\n",
    "test_dir = 'CNN_dataset/test'    # Path to the test folder\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check the structure of the dataset\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f'Batch of images shape: {images.shape}')  # Should be (batch_size, 3, 128, 128)\n",
    "    print(f'Batch of labels shape: {labels.shape}')  # Should be (batch_size,)\n",
    "    break  # Just show the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from albumentations import Compose, Resize, HorizontalFlip, Rotate, RandomBrightnessContrast\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the Albumentations transformations\n",
    "def albumentations_transform(image):\n",
    "    # Apply random horizontal flip, random crop, and random brightness/contrast\n",
    "    transform = Compose([\n",
    "        HorizontalFlip(p=0.5),  # Flip the image horizontally with 50% probability\n",
    "        # RandomCrop(width=120, height=120, p=1),  # Crop the image randomly to 120x120\n",
    "        Rotate(limit=30, p=0.5),  # Rotate the image randomly by up to 30 degrees\n",
    "        RandomBrightnessContrast(p=0.2),  # Randomly change brightness/contrast\n",
    "        ToTensorV2()    # Convert image to PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformations to the image (albumentations works on NumPy arrays)\n",
    "    image = np.array(image)  # Convert PIL image to NumPy array\n",
    "    augmented = transform(image=image)\n",
    "    return augmented['image']\n",
    "\n",
    "# Define the paths to the dataset directories\n",
    "train_dir = 'CNN_dataset/train'  # Path to the train folder\n",
    "test_dir = 'CNN_dataset/test'    # Path to the test folder\n",
    "\n",
    "# Define the custom dataset class with Albumentations transform\n",
    "class CustomImageFolderWithAugmentations(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset class for loading images from a folder with augmentations.\n",
    "        \n",
    "        :param image_folder: Path to the dataset folder containing subfolders for each class.\n",
    "        :param transform: Albumentations transformation pipeline to apply to images.\n",
    "        \"\"\"\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load image paths and labels\n",
    "        for label, folder_name in enumerate(os.listdir(image_folder)):\n",
    "            folder_path = os.path.join(image_folder, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for img_name in os.listdir(folder_path):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image using PIL\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Convert image to numpy array for Albumentations\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Apply transformations (augmentation) using Albumentations if provided\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image_np)\n",
    "            image = augmented['image']  # This will be a PyTorch tensor\n",
    "\n",
    "        # Ensure the image is of type float32 and normalize to [0, 1]\n",
    "        # Convert from uint8 to float32\n",
    "        image = image.float()  # Convert tensor to float32\n",
    "\n",
    "        # Normalize pixel values to [0, 1] by dividing by 255.0\n",
    "        image /= 255.0\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Images dtype: torch.float32\n",
      "Images range: 0.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the augmentation pipeline\n",
    "augmentation_pipeline = Compose([\n",
    "    Resize(224, 224),\n",
    "    # HorizontalFlip(p=0.5),\n",
    "    # VerticalFlip(p=0.5),\n",
    "    # RandomCrop(width=120, height=120, p=1),\n",
    "    # Rotate(limit=30, p=0.5),\n",
    "    # RandomBrightnessContrast(p=0.2),\n",
    "    ToTensorV2()  # Converts to a PyTorch tensor, but does NOT normalize to [0, 1] yet\n",
    "])\n",
    "\n",
    "# Path to the dataset folder containing subfolders 'real' and 'fake' (or any other class names)\n",
    "dataset_folder = 'dataset'\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = CustomImageFolderWithAugmentations(image_folder=dataset_folder, transform=augmentation_pipeline)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f'Batch size: {images.shape[0]}')  # Should print batch size (32)\n",
    "    print(f'Images dtype: {images.dtype}')   # Should be torch.float32\n",
    "    print(f'Images range: {images.min()}, {images.max()}')  # Should be in the range [0, 1]\n",
    "    break  # Just print information about the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16Custom(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGG16Custom(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(VGG16Custom, self).__init__()\n",
    "        \n",
    "        # VGG16-like Convolutional layers with max pooling\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # 224x224 -> 224x224\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # 224x224 -> 224x224\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 224x224 -> 112x112\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # 112x112 -> 112x112\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # 112x112 -> 112x112\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 112x112 -> 56x56\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)  # 56x56 -> 56x56\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)  # 56x56 -> 56x56\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)  # 56x56 -> 56x56\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 56x56 -> 28x28\n",
    "\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
    "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
    "\n",
    "        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)  # 14x14 -> 14x14\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)  # 14x14 -> 14x14\n",
    "        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1)  # 14x14 -> 14x14\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7\n",
    "\n",
    "        # Fully connected layers (FC1: 4096 units, FC2: 4096 units, FC3: num_classes)\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)  # Adjusted for 224x224 input (512 * 7 * 7 = 25088)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        # Dropout (optional)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activations and max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 512 * 7 * 7)  # For input size of 224x224\n",
    "\n",
    "        # Fully connected layers with ReLU activation and Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Sigmoid activation for binary classification\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        # Reshape to (batch_size,) for binary classification target matching\n",
    "        x = x.view(-1)  # Flatten to (batch_size,)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model for binary classification (1 output unit)\n",
    "model = VGG16Custom(num_classes=1)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up the criterion (loss function) and optimizer\n",
    "# Loss function for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 (Train): 100%|██████████| 64/64 [03:58<00:00,  3.72s/batch, accuracy=49.1, loss=18.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 18.6874, Accuracy: 49.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 (Train): 100%|██████████| 64/64 [03:08<00:00,  2.95s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Loss: 47.0117, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.03s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Loss: 46.9844, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 (Train): 100%|██████████| 64/64 [03:15<00:00,  3.06s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Loss: 47.0391, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Loss: 46.9844, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 (Train): 100%|██████████| 64/64 [03:11<00:00,  3.00s/batch, accuracy=53, loss=47.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Loss: 47.0527, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Loss: 47.0391, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 (Train): 100%|██████████| 64/64 [03:15<00:00,  3.05s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Loss: 47.0391, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Loss: 47.0254, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Loss: 47.0254, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.03s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Loss: 46.9570, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 (Train): 100%|██████████| 64/64 [03:09<00:00,  2.96s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Loss: 46.9980, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 (Train): 100%|██████████| 64/64 [02:51<00:00,  2.67s/batch, accuracy=53, loss=47.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Loss: 47.0527, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 (Train): 100%|██████████| 64/64 [02:51<00:00,  2.68s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Loss: 47.0254, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 (Train): 100%|██████████| 64/64 [03:13<00:00,  3.02s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Loss: 47.0391, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Loss: 47.0801, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Loss: 46.9844, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 (Train): 100%|██████████| 64/64 [03:14<00:00,  3.04s/batch, accuracy=53, loss=47.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Loss: 47.0664, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 (Train): 100%|██████████| 64/64 [03:13<00:00,  3.03s/batch, accuracy=53, loss=47.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Loss: 47.0527, Accuracy: 52.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 (Train): 100%|██████████| 64/64 [03:07<00:00,  2.92s/batch, accuracy=53, loss=47]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Loss: 47.0254, Accuracy: 52.96%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Wrap the train_loader with tqdm for the training loop progress bar\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\") as tepoch:\n",
    "        for inputs, labels in tepoch:\n",
    "#     for inputs, labels in train_loader:  \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Logits: (batch_size, 1) for binary classification\n",
    "            # print(inputs.shape, labels.shape)\n",
    "\n",
    "            # break\n",
    "#             # Calculate the loss (using BCEWithLogitsLoss for binary classification)\n",
    "            # print(outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate the number of correct predictions\n",
    "            predicted = (outputs > 0.5).float()  # Apply threshold of 0.5 to get binary predictions (0 or 1)\n",
    "            correct_preds += (predicted.squeeze() == labels).sum().item()  # Compare predictions with true labels\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "            # Update the tqdm progress bar description with loss and accuracy\n",
    "            tepoch.set_postfix(loss=running_loss / len(tepoch), accuracy=100 * correct_preds / total_preds)\n",
    "#     Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct_preds / total_preds\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    torch.cuda.empty_cache()\n",
    "torch.save(model.state_dict(), 'vgg16_custom_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
